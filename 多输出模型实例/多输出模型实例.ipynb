{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多输出模型实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集准备\n",
    "data_dir = r'D:\\dataset\\multi-output-classification\\dataset'\n",
    "colors = ['black', 'blue', 'red']\n",
    "coats = ['jeans', 'shoes', 'dress', 'shirt']\n",
    "all_dirs = os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames = []\n",
    "color_labels = []\n",
    "coat_labels = []\n",
    "for each_dir in all_dirs:\n",
    "    color, coat = each_dir.split('_')\n",
    "    color_id, coat_id = colors.index(color), coats.index(coat)\n",
    "    image_docunts = len(os.listdir(os.path.join(data_dir, each_dir)))\n",
    "    color_labels.extend([[color_id]] * image_docunts)\n",
    "    coat_labels.extend([[coat_id]] * image_docunts)\n",
    "    image_filenames.extend(glob(os.path.join(data_dir, each_dir, '*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(x, y):\n",
    "    image = tf.io.read_file(x)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, size=(height, width))\n",
    "#     image = tf.keras.applications.inception_v3.preprocess_input(image)\n",
    "    image = image / 255.0 * 2 - 1\n",
    "    return image, y\n",
    "\n",
    "height = 224\n",
    "width = 224\n",
    "channels = 3\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_filenames, (color_labels, coat_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割训练集和测试集\n",
    "train_count = int(len(image_filenames) * 0.8)\n",
    "train_dataset = dataset.take(train_count)\n",
    "valid_dataset = dataset.skip(train_count)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(train_count).map(preprocessing).batch(batch_size).repeat()\n",
    "valid_dataset = valid_dataset.map(preprocessing).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mobilenetv2_1.00_224 (Model)    (None, 1280)         2257984     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 3)            3843        mobilenetv2_1.00_224[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            5124        mobilenetv2_1.00_224[1][0]       \n",
      "==================================================================================================\n",
      "Total params: 2,266,951\n",
      "Trainable params: 2,232,839\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "# base = tf.keras.applications.inception_v3.InceptionV3(\n",
    "#     include_top=False, weights='imagenet', input_shape=(height, width, channels),\n",
    "#     pooling='avg'\n",
    "# )\n",
    "# base = tf.keras.applications.ResNet50(\n",
    "#     include_top=False, weights='imagenet', input_shape=(height, width, channels),\n",
    "#     pooling='avg'\n",
    "# )\n",
    "base = tf.keras.applications.MobileNetV2(\n",
    "    include_top=False, weights='imagenet', input_shape=(height, width, channels),\n",
    "    pooling='avg'\n",
    ")\n",
    "\n",
    "base.trainable = True\n",
    "x = keras.layers.Input(shape=(height, width, channels))\n",
    "out = base(x)\n",
    "# out1为颜色分类输出\n",
    "# out1 = keras.layers.Dense(512, activation='relu')(out)\n",
    "# out1 = keras.layers.Dense(2048, activation='relu')(out1)\n",
    "out1 = keras.layers.Dense(len(colors), activation='softmax')(out)\n",
    "# out2为衣服种类分类输出\n",
    "# out2 = keras.layers.Dense(1024, activation='relu')(out)\n",
    "out2 = keras.layers.Dense(len(coats), activation='softmax')(out)\n",
    "\n",
    "model = keras.models.Model(x, (out1, out2))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset.take(1).as_numpy_iterator().next()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置模型\n",
    "model.compile(\n",
    "    loss=keras.losses.sparse_categorical_crossentropy,\n",
    "    optimizer=keras.optimizers.Adam(lr=1e-4),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=15),\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=10),\n",
    "    keras.callbacks.ModelCheckpoint('./multi_classifier.tf', save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "252/252 [==============================] - 43s 171ms/step - loss: 0.0075 - dense_loss: 0.0040 - dense_1_loss: 0.0035 - dense_accuracy: 0.9995 - dense_1_accuracy: 0.9985 - val_loss: 0.6917 - val_dense_loss: 0.3265 - val_dense_1_loss: 0.3652 - val_dense_accuracy: 0.8812 - val_dense_1_accuracy: 0.9228 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "252/252 [==============================] - 42s 167ms/step - loss: 0.0086 - dense_loss: 0.0046 - dense_1_loss: 0.0040 - dense_accuracy: 0.9980 - dense_1_accuracy: 0.9985 - val_loss: 0.7213 - val_dense_loss: 0.4521 - val_dense_1_loss: 0.2691 - val_dense_accuracy: 0.8337 - val_dense_1_accuracy: 0.9406 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "252/252 [==============================] - 42s 168ms/step - loss: 0.0029 - dense_loss: 0.0016 - dense_1_loss: 0.0012 - dense_accuracy: 1.0000 - dense_1_accuracy: 1.0000 - val_loss: 0.7846 - val_dense_loss: 0.5264 - val_dense_1_loss: 0.2582 - val_dense_accuracy: 0.8198 - val_dense_1_accuracy: 0.9446 - lr: 1.0000e-05\n",
      "Epoch 4/100\n",
      "252/252 [==============================] - 43s 171ms/step - loss: 0.0025 - dense_loss: 0.0015 - dense_1_loss: 0.0010 - dense_accuracy: 0.9995 - dense_1_accuracy: 1.0000 - val_loss: 0.7930 - val_dense_loss: 0.5329 - val_dense_1_loss: 0.2601 - val_dense_accuracy: 0.8178 - val_dense_1_accuracy: 0.9446 - lr: 1.0000e-05\n",
      "Epoch 5/100\n",
      "252/252 [==============================] - 48s 189ms/step - loss: 0.0021 - dense_loss: 0.0014 - dense_1_loss: 7.5417e-04 - dense_accuracy: 1.0000 - dense_1_accuracy: 1.0000 - val_loss: 0.7832 - val_dense_loss: 0.5136 - val_dense_1_loss: 0.2696 - val_dense_accuracy: 0.8238 - val_dense_1_accuracy: 0.9426 - lr: 1.0000e-05\n",
      "Epoch 6/100\n",
      "252/252 [==============================] - 45s 178ms/step - loss: 0.0036 - dense_loss: 0.0019 - dense_1_loss: 0.0017 - dense_accuracy: 0.9995 - dense_1_accuracy: 0.9995 - val_loss: 0.7975 - val_dense_loss: 0.4920 - val_dense_1_loss: 0.3055 - val_dense_accuracy: 0.8356 - val_dense_1_accuracy: 0.9347 - lr: 1.0000e-05\n",
      "Epoch 7/100\n",
      "252/252 [==============================] - 44s 176ms/step - loss: 0.0029 - dense_loss: 0.0023 - dense_1_loss: 6.5564e-04 - dense_accuracy: 0.9995 - dense_1_accuracy: 1.0000 - val_loss: 0.8426 - val_dense_loss: 0.5476 - val_dense_1_loss: 0.2950 - val_dense_accuracy: 0.8139 - val_dense_1_accuracy: 0.9386 - lr: 1.0000e-05\n",
      "Epoch 8/100\n",
      "252/252 [==============================] - 45s 177ms/step - loss: 0.0023 - dense_loss: 0.0012 - dense_1_loss: 0.0010 - dense_accuracy: 1.0000 - dense_1_accuracy: 0.9995 - val_loss: 0.8089 - val_dense_loss: 0.4957 - val_dense_1_loss: 0.3132 - val_dense_accuracy: 0.8277 - val_dense_1_accuracy: 0.9347 - lr: 1.0000e-05\n",
      "Epoch 9/100\n",
      "252/252 [==============================] - 45s 178ms/step - loss: 0.0043 - dense_loss: 0.0035 - dense_1_loss: 7.2841e-04 - dense_accuracy: 0.9990 - dense_1_accuracy: 1.0000 - val_loss: 0.8535 - val_dense_loss: 0.5651 - val_dense_1_loss: 0.2884 - val_dense_accuracy: 0.8020 - val_dense_1_accuracy: 0.9406 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "252/252 [==============================] - 49s 193ms/step - loss: 0.0033 - dense_loss: 0.0012 - dense_1_loss: 0.0022 - dense_accuracy: 1.0000 - dense_1_accuracy: 0.9995 - val_loss: 0.8524 - val_dense_loss: 0.5452 - val_dense_1_loss: 0.3072 - val_dense_accuracy: 0.8139 - val_dense_1_accuracy: 0.9386 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "252/252 [==============================] - 50s 200ms/step - loss: 0.0089 - dense_loss: 0.0086 - dense_1_loss: 3.7521e-04 - dense_accuracy: 0.9975 - dense_1_accuracy: 1.0000 - val_loss: 0.8868 - val_dense_loss: 0.6131 - val_dense_1_loss: 0.2737 - val_dense_accuracy: 0.8020 - val_dense_1_accuracy: 0.9446 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "252/252 [==============================] - 49s 196ms/step - loss: 0.0061 - dense_loss: 0.0053 - dense_1_loss: 7.7280e-04 - dense_accuracy: 0.9985 - dense_1_accuracy: 1.0000 - val_loss: 0.8493 - val_dense_loss: 0.5741 - val_dense_1_loss: 0.2752 - val_dense_accuracy: 0.8059 - val_dense_1_accuracy: 0.9426 - lr: 1.0000e-06\n",
      "Epoch 13/100\n",
      "252/252 [==============================] - 48s 189ms/step - loss: 0.0033 - dense_loss: 0.0027 - dense_1_loss: 5.8981e-04 - dense_accuracy: 0.9990 - dense_1_accuracy: 1.0000 - val_loss: 0.8325 - val_dense_loss: 0.5568 - val_dense_1_loss: 0.2757 - val_dense_accuracy: 0.8099 - val_dense_1_accuracy: 0.9406 - lr: 1.0000e-06\n",
      "Epoch 14/100\n",
      "252/252 [==============================] - 47s 188ms/step - loss: 0.0026 - dense_loss: 0.0017 - dense_1_loss: 8.4568e-04 - dense_accuracy: 1.0000 - dense_1_accuracy: 1.0000 - val_loss: 0.8260 - val_dense_loss: 0.5504 - val_dense_1_loss: 0.2756 - val_dense_accuracy: 0.8099 - val_dense_1_accuracy: 0.9406 - lr: 1.0000e-06\n",
      "Epoch 15/100\n",
      "252/252 [==============================] - 48s 190ms/step - loss: 0.0023 - dense_loss: 0.0014 - dense_1_loss: 8.5226e-04 - dense_accuracy: 1.0000 - dense_1_accuracy: 1.0000 - val_loss: 0.8223 - val_dense_loss: 0.5450 - val_dense_1_loss: 0.2773 - val_dense_accuracy: 0.8119 - val_dense_1_accuracy: 0.9406 - lr: 1.0000e-06\n",
      "Epoch 16/100\n",
      "252/252 [==============================] - 46s 181ms/step - loss: 0.0021 - dense_loss: 0.0013 - dense_1_loss: 7.8766e-04 - dense_accuracy: 1.0000 - dense_1_accuracy: 1.0000 - val_loss: 0.8204 - val_dense_loss: 0.5444 - val_dense_1_loss: 0.2761 - val_dense_accuracy: 0.8119 - val_dense_1_accuracy: 0.9406 - lr: 1.0000e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bd4ddd5b08>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=100, steps_per_epoch=train_count // batch_size,\n",
    "          validation_data=valid_dataset, callbacks=callbacks\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
